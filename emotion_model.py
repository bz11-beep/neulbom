"""
ì ì‘í˜• ê°ì • í†µí•© ì—”ì§„ (Adaptive Emotion Fusion) - ê³ ì† ìµœì í™” ë²„ì „
- Pitch ë¶„ì„ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” (yin ì‚¬ìš©)
- ì˜¤ë””ì˜¤ ìƒ˜í”Œë§ ë° ë¡œë“œ êµ¬ê°„ ì œí•œ (3ì´ˆ)
- ë©€í‹°ëª¨ë‹¬ ì•™ìƒë¸” ìœ ì§€
"""

import torch
import torch.nn.functional as F
import librosa
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification
from config.models import MODELS

class EmotionEnsemble:
    """
    ê°œì„ ëœ ê°ì • ë¶„ì„ ì—”ì§„ (ìµœì í™” ë²„ì „)
    - Z-score ê¸°ë°˜ Pitch Dynamics ë¶„ì„ (ê³ ì†)
    - í…ìŠ¤íŠ¸(KcELECTRA) + ìŒì„±(Wav2Vec2) ë©€í‹°ëª¨ë‹¬ ê²°í•©
    """
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"â¤ï¸â€ðŸ©¹ ê³ ì† ê°ì • ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™” (Device: {self.device})")

        try:
            # Configì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°
            text_model_name = MODELS['emotion_text']
            audio_model_name = MODELS['emotion_audio']
            
            # 1. í…ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ
            self.text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)
            self.text_model = AutoModelForSequenceClassification.from_pretrained(text_model_name).to(self.device)
            
            # 2. ìŒì„± ëª¨ë¸ ë¡œë“œ
            self.audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_name)
            self.audio_model = Wav2Vec2ForSequenceClassification.from_pretrained(audio_model_name).to(self.device)
            
            self.text_model.eval()
            self.audio_model.eval()
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e

    def predict(self, audio_path, text):
        """
        [ê³ ì† ë²„ì „] ìŒì„± íŒŒì¼ê³¼ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ ê°ì • ë¶„ì„
        """
        try:
            # ðŸš€ ìµœì í™”: 16kHzë¡œ ë¦¬ìƒ˜í”Œë§í•˜ë©° ì•žë¶€ë¶„ ìµœëŒ€ 3ì´ˆë§Œ ë¡œë“œ (ë³‘ëª© ì œê±°)
            y, sr = librosa.load(audio_path, sr=16000, duration=3.0)
            
            # 1. ê³ ì† Pitch ë¶„ì„ (Z-peak)
            z_peak = self._calculate_pitch_zscore(y, sr)
            
            # 2. ìŒì„± ê°ì • ë¶„ì„ (Wav2Vec2)
            audio_inputs = self.audio_processor(y, sampling_rate=sr, return_tensors="pt").to(self.device)
            with torch.no_grad():
                audio_logits = self.audio_model(**audio_inputs).logits
            
            audio_probs = F.softmax(audio_logits, dim=-1)
            audio_conf, audio_idx = torch.max(audio_probs, dim=-1)
            audio_label = self._translate_audio(self.audio_model.config.id2label[audio_idx.item()])

            # 3. í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ (KcELECTRA)
            text_inputs = self.text_tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                max_length=128
            ).to(self.device)
            
            with torch.no_grad():
                text_logits = self.text_model(**text_inputs).logits
            
            text_probs = F.softmax(text_logits, dim=-1)
            text_conf, text_idx = torch.max(text_probs, dim=-1)
            text_label = self.text_model.config.id2label[text_idx.item()]

            # 4. ë©€í‹°ëª¨ë‹¬ ê°€ì¤‘ì¹˜ ê²°í•© (PDF ê¸°ìˆ ë…¸íŠ¸ ê¸°ë°˜ ë¡œì§)
            # ê¸°ë³¸ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ê°ì •ì„ ë”°ë¥´ë˜, ëª©ì†Œë¦¬ í†¤(Z-peak)ì´ ê°•í•˜ë©´ ìŒì„± ê°ì • ë°˜ì˜
            final_emotion = text_label
            
            # ê·œì¹™: ëª©ì†Œë¦¬ì— ê°ì • ë³€í™”ê°€ í¬ê³ (Z-peak ê°€ ë†’ê³ ) ìŒì„±ì´ í™•ì‹¤í•  ë•Œ
            if z_peak > 2.5 and audio_label in ['ë¶„ë…¸', 'ê¸°ì¨', 'ìŠ¬í””']:
                # í…ìŠ¤íŠ¸ê°€ ì¤‘ë¦½ì´ê±°ë‚˜ ìŒì„± ì‹ ë¢°ë„ê°€ ë†’ì„ ë•Œ êµì²´
                if text_label == 'ì¤‘ë¦½' or audio_conf > 0.6:
                    final_emotion = audio_label

            return {
                'final_emotion': final_emotion,
                'text_label': text_label,
                'audio_label': audio_label,
                'z_peak': float(z_peak),
                'audio_conf': float(audio_conf),
                'text_conf': float(text_conf)
            }

        except Exception as e:
            print(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                'final_emotion': 'ì¤‘ë¦½',
                'text_label': 'ì¤‘ë¦½',
                'audio_label': 'ì¤‘ë¦½',
                'z_peak': 0.0,
                'audio_conf': 0.0
            }

    def _calculate_pitch_zscore(self, y, sr, sigma_min=5.0):
        """
        ðŸš€ ì´ˆê³ ì† Pitch ë¶„ì„ (YIN ì•Œê³ ë¦¬ì¦˜ ì ìš©)
        """
        try:
            # librosa.yinì€ pyinë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„
            f0 = librosa.yin(y, fmin=65, fmax=500, sr=sr, frame_length=1024)
            
            # ìœ íš¨í•œ í”¼ì¹˜ ê°’ë§Œ ì¶”ì¶œ
            f0_valid = f0[f0 > 0]
            if len(f0_valid) < 5:
                return 0.0
            
            # Z-score ê³„ì‚° (ëª©ì†Œë¦¬ì˜ ì—­ë™ì„± ì¸¡ì •)
            mu_f0 = np.mean(f0_valid)
            sigma_f0 = np.std(f0_valid)
            sigma_safe = max(sigma_f0, sigma_min)
            
            z_scores = np.abs((f0_valid - mu_f0) / sigma_safe)
            return float(np.max(z_scores))
            
        except:
            return 0.0

    def _translate_audio(self, label):
        """ìŒì„± ê°ì • ë ˆì´ë¸” í•œê¸€ ë§¤í•‘"""
        label = str(label).lower()
        mapping = {
            'angry': 'ë¶„ë…¸', 'fear': 'ë¶ˆì•ˆ', 'happy': 'ê¸°ì¨', 
            'neutral': 'ì¤‘ë¦½', 'sad': 'ìŠ¬í””', 'surprise': 'ë‹¹í™©',
            '0': 'ë¶„ë…¸', '1': 'ê¸°ì¨', '2': 'ë¶ˆì•ˆ', '3': 'ìŠ¬í””', '4': 'ì¤‘ë¦½'
        }
        for k, v in mapping.items():
            if k in label:
                return v
        return 'ì¤‘ë¦½'